# PS 1 - Espaces probabilistes

Etherpad : <http://pads.univ-grenoble-alpes.fr/p/INFO4_PS_19>

$$
(\Omega, \mathcal{A}, \mathbb{P})
$$

## Rappels

- Univers des évènements : $$\Omega = \{\omega_1, \omega_2, \omega_3, ...\}$$
- Évènement $$A \in \Omega$$
- $$\mathcal{A}$$ : tous les $$A$$ considérés
- $$\mathbb{P} = \begin{array}{l}
    \mathcal{A} & \rightarrow & [0, 1]\\
    A & \mapsto & \mathbb{P}[A]
\end{array}$$
- $$\mathbb{P}(\Omega) = 1$$
- $$\mathbb{P}(\emptyset) = 0$$
- $$\mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) - \mathbb{P}(A \cap B)$$
- 2 dés 6 : $$\Omega = \{(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), ..., (6, 6)\}$$
  - Probabilité qu'on ait un double : $$\forall v \in \Omega : \mathbb{P}(v) = {1 \over |\Omega|} = {1 \over 36}$$

## Zombie Dice

|   3R    |  4Y   |  6G   |
| :-----: | :---: | :---: |
| 3 bang  |   2   |   1   |
| 2 brain |   2   |   3   |
| 1 walk  |   2   |   2   |

- $$A = \{R, Y, G\}$$
- $$\Omega = A^3$$

> `sample(x = c("Bang", "Brain", "Walk"), size = 1, prob = c(3, 2, 1), replace = T)`
>
> `sample(x = c(rep("R", 3), rep("Y", 4), rep("G", 6)), size = 3, replace = F)`

## Espace continu

Pour n'importe quel point de $$\Omega = [0, 1]$$ sa probabilité est nulle (si on sommait la probabilité de chacun des éléments, dont il y a une infinité, on aurait un souci).

**Dénombrable** / indénombrable = qu'on peut / ne peut pas compter

- $$\mathbb{P}([a, b]) = |b - a|$$
- $$\mathbb{P}(\{a\}) = 0$$
- $$\mathbb{P}(\mathbb{Q}) = ?$$

## Variable aléatoire $$X$$

$$X: \Omega \rightarrow \mathbb{R}|\mathbb{N}$$

$$
\forall x \in \mathbb{R} (\text{ou } \mathbb{N}) : \{\omega / X(\omega) \leq x \} \in \mathcal{A}
$$

$$
F_X : \begin{array}{l}
    \mathbb{F} & \rightarrow & [0, 1]\\
    x & \rightarrow & \mathbb{P}(X(\omega) \leq x)
\end{array}
$$

(fonction de répartition : cumulative distribution function / CDF )

- $$f_X : \mathbb{R} \rightarrow \mathbb{R}$$
- $$f_X = F_X'$$
- $$F_X(x) = \int\limits_{-\infty}^{x} f_X(u)du$$
- $$f_X \simeq \mathbb{P}$$

Espérance (*expected value*) :

- Discret : $$\mathbb{E}(X) = \sum\limits_{i = 1}^{n} x_i \mathbb{P}(X=x_i)$$
- Continu (observé) : $$\mathbb{E}(X) = \int\limits_{-\infty}^{+\infty} xf_X(x)dx$$
- Continu (toutes les possibilités) : $$\mathbb{E}(X) = \int\limits_{-\omega} X(\omega)d\mathbb{P}(\omega)$$

> On peut noter $$\mathbb{P}(X=x_i)$$ en $$p_i$$

*Exemple.* Prenons un dé 6.

- $$"\Omega" = \{\cdot, \cdot\cdot, \cdots, \cdots\cdots\}$$
- $$X(\Omega) = \{1, 2, 3, 4, 5, 6\}$$
- $$\mathbb{P}(X=k) = \left\{\begin{array}{lcl}
    1/6 \text{ si } k \in \{1, 2, ..., 6\}\\
    0 \text{ sinon}
    \end{array}\right.$$
- $$\begin{array}{lcl}
  \mathbb{E}(X) & = & {1 \over 6} \times 1 + {1 \over 6} \times 2 + ... + {1 \over 6} \times 6\\
  & = & {1 \over 6} \times (1 + 2 + ... + 6)\\
  & = & {21 \over 6} = {7 \over 2}\\
  \end{array}$$

*Exemple.* Prenons deux dés 6.

- $$"\Omega" = \{(a, b)\; |\; a \in \{1, ..., 6\} \text{ et } b \in \{1, ..., 6\}\}$$
- $$X(\Omega) = \{2, 3, ..., 12\}$$
- $$\mathbb{P}(X=k) = \{ {1 \over 36}, {2 \over 36}, {3 \over 36}, ..., {6 \over 36}, {5 \over 36}, ..., {1 \over 36}\}$$
- $$\sum = {1 \over 36}(1+2+3+4+5+6+5+4+3+2+1) = {1 \over 36}(15 + 6 + 15) = 1$$
- $$\begin{array}{lcl}
  \mathbb{E}(X) & = & {1 \over 36} \times (1 \times 2 + 2 \times 3 + ... + 5 \times 6 + 6 \times 7 + 8 \times 5 + ... + 12 \times 1)\\
  \end{array}$$


*Exemple.* Pile ou Face

- $$"\Omega" = \{\text{Pile}, \text{Face}\}$$
- $$X(\Omega) = \{0, 1\}$$
- $$\mathbb{P}(X=k) = {1 \over 2}$$
- $$\mathbb{E}(X) = {1 \over 2} \times 0 + {1 \over 2} \times 1 = {1 \over 2}$$

*Exemple.* Loterie, 2D6, double 6 $$\rightarrow$$ 1M€, sinon -10K€

- $$\begin{array}{lcl}
  \mathbb{E}(X) & = & {1 \over 36} \times 10^6 - {35 \over 36} \times 10^4\\
  & \approx & 18000 \text{Euros} \\
  \end{array}$$


$$\text{Unif}(0, 1)$$

- $$X(\Omega) = [0, 1]$$
- $$f_X(x) =  \left\{\begin{array}{lcl}
    1 \text{ si } x \in [0, 1]\\
    0 \text{ sinon}
    \end{array}\right.$$
- $$f_X(x) : \mathbb{R} \rightarrow \mathbb{R}^+$$
- $$\begin{array}{lcl}
    \mathbb{E}(X) & = & \int_{- \infty}^{+ \infty} x \times f_X(x)dx \text{ (}f_X(x) = 0\text{ ou }1\text{)}\\
    & = & \int_0^1 xdx\\
    & = & [{x^2 \over 2}]^1_0\\
    & = & {1 \over 2}
    \end{array}$$

$$\text{Unif}(0, 1)^2$$

- $$X^2(\Omega) = [0, 1]$$
- $$\begin{array}{lcl}
    \mathbb{E}(X^2) & = & \int_0^1 uf_{X^2}(u)du\\
    & = & \int_0^1 x^2f_X(x)dx \text{ (}f_X(x) = 1\text{ entre } 0 \text{ et } 1\text{)}\\
    & = & [{x^3 \over 3}]^1_0\\
    & = & {1 \over 3}
    \end{array}$$


$$X \sim E(\lambda)$$

- $$f_X(x) = \left\{\begin{array}{lcl}
    \lambda \times e^{- \lambda x} \text{ si } x \geq 0\\
    0 \text{ sinon}
    \end{array}\right.$$
- On vérifie que la formule soit correcte :

    $$\begin{array}{lcl}
    \mathbb{P}(\Omega) & = & \int_0^{\infty} \lambda e^{\lambda x} dx\\
    & = & [{-e^{\lambda x} \lambda \over \lambda}]_0^{\infty}\\
    & = & 0 - {-1 \lambda \over \lambda}\\
    & = & 1\\
    \end{array}$$

Cauchy : $$f_X(x) = \alpha {1 \over 1 + x^2} \text{ si } x \geq 0, 0 \text{ sinon}$$, $$E(X) = \int_0^{\infty} {\alpha x \over 1 + x^2}$$ (on ne peut pas la calculer)

## Espérance

### Formules

$$
(\Omega, A, \mathbb{P})
$$

$$
\begin{array}{lcl}
X & : & \Omega \rightarrow \mathbb{R}\\
f_X & : & \mathbb{R} \rightarrow \mathbb{R}^+\\
F_X & : & \mathbb{R} \rightarrow [0, 1]\\
\end{array}
$$

$$
\begin{array}{lcl}
\mathbb{E}(X) & = & \int_{-\infty}^{+\infty} xf_X(x) dx\\
 & = & \int X(\omega) d \mathbb{P}(\omega)\\
\mathbb{E}(aX) & = & a \mathbb{E}(X)\\
\mathbb{E}(X + Y)& = & \mathbb{E}(X) + \mathbb{E}(Y)\\
\mathbb{E}(X \times Y)& = & \text{cov}(XY) \text{ cov = covariance}\\
\end{array}
$$

### Démonstrations

$$
aX : \left\{\begin{array}{rcl} \Omega & \rightarrow & \mathbb{R}\\\omega & \mapsto & aX(\omega)\end{array}\right.
$$

$$
f_{aX}(x) = {1 \over a}f_X({x \over a})
$$

$$
\begin{array}{lcl}
\mathbb{E}(aX) & = & \int_{-\infty}^{+\infty} xf_{aX}(x) dx\\
& = & \int_{-\infty}^{+\infty} {x \over a} f_{X}({x \over a}) dx\\
& = & \int_{-\infty}^{+\infty} y f_{X}(y) dy \text{ avec } y = {x \over a}\\
& = & a \mathbb{E}(X)\\
\end{array}
$$

$$
\begin{array}{lcl}
\mathbb{E}(aX) & = & \int_{-\infty}^{+\infty} (aX)(\omega) d\mathbb{P}(\omega)\\
& = & \int_{-\infty}^{+\infty} a \times X(\omega) d\mathbb{P}(\omega)\\
& = & a \int_{-\infty}^{+\infty} \lambda (\omega) d\mathbb{P}(\omega)\\
& = & a \mathbb{E}(X)\\
\end{array}
$$

$$
\begin{array}{lcl}
\mathbb{E}(X + Y) & = & \int_{-\infty}^{+\infty} (X + Y)(\omega) d\mathbb{P}(\omega)\text{ avec } (X+Y)(\omega) = X(\omega) + Y(\omega)\\
& = & \int_{-\infty}^{+\infty} X(\omega)d\mathbb{P}(\omega) + \int_{-\infty}^{+\infty} Y(\omega)d\mathbb{P}(\omega)\\
& = & \mathbb{E}(X) + \mathbb{E}(Y)\\
\end{array}
$$

## Variance

### Formules

Soit $$X$$ tel que $$\mathbb{E}(X) = \mu$$ :

$$
\begin{array}{lcl}
\text{Var}(X) & = & \mathbb{E}((X - \mu)^2) \Big(\leadsto \sigma(X) = \sqrt{\text{Var}(x)}\Big)\\
& = & \mathbb{E}(X^2 - 2 \mu X + \mu^2)\\
& = & \mathbb{E}(X^2) - 2 \mu \mathbb{E}(X) + \mu^2\\
& = & \mathbb{E}(X^2) - 2 \mu \times \mu + \mu^2\\
& = & \mathbb{E}(X^2) - \mu^2\\
\end{array}
$$

$$
\begin{array}{lcl}
\text{Var}(aX) & = & \mathbb{E}(X^2) - \mu^2\\
\text{Var}(X - a) & = & \text{Var}(X) \\
\text{Var}(X + Y) & = & \text{Var}(X) + \text{Var}(Y) + 2 \mathbb{E}(XY) \\
\end{array}
$$

### Démonstrations

$$
\begin{array}{lcl}
\text{Var}(aX) & = & \mathbb{E}(((aX)^2) - (\mathbb{E}(aX))^2\\
& = & a^2 \mathbb{E}(X^2) - (a\mathbb{E}(X))^2\\
& = & a^2 (\mathbb{E}(X^2) - \mathbb{E}(X))^2\\
\end{array}
$$

$$
\begin{array}{lcl}
\text{Var}(X + Y) & = & \mathbb{E}((X + Y)^2)\\
& = & \mathbb{E}(X^2 + Y^2 + 2XY)\\
& = & \text{Var}(X) + \text{Var}(Y) + 2\mathbb{E}(XY) \\
\end{array}
$$

## Indépendance et probabilité conditionnelle

**Définition.** Deux évènements sont indépendants si $$P(A, B) = P(A) P(B)$$

*Exemple.* Course avec 3 chevaux :

- Ensemble des probabilités : $$\Omega = \{(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)\}$$
- Le cheval 1 gagne : $$A = \{(1, 2, 3), (1, 3, 2)\}$$
- Le cheval 2 devant le cheval 3 : $$B = \{1, 2, 3\}, \{2, 1, 3\}, \{2, 3, 1\}$$
- $$P(A) = {1 \over 3}$$, $$P(B) = {1 \over 2}$$
- $$A$$ et $$B$$ sont indépendants : $$P(A, B) = P(A \cap B) = {1 \over 3} \times {1 \over 2} = {1 \over 6}$$
- $$A_1 ... A_n$$ mutuellement indépendants si pour tout sous-ensemble de $$A_1 ... A_n$$, $$P(\cap) = \prod \text{Proba}$$

*Exemple.* $$n = 3$$ : $$A_1, A_2, A_3$$

Si $A_1, A_2, A_3$ indépendantes, alors 

- $$P(A_1 \cap A_2) = P(A_1) . P(A_2)$$
- $$P(A_1 \cap A_3) = P(A_1) . P(A_3)$$
- $$P(A_2 \cap A_3) = P(A_2) . P(A_3)$$
- $$P(A_1 \cap A_2 \cap A_3) = P(A_1) . P(A_2) . P(A_3)$$

> $$P(A_1 \cap A_2 \cap A_3 = P(A_1) . P(A_2) . P(A_3)$$ ne suffit pas.
>
> Pas transitive. $A\; \text{indep } B$ et $B\; \text{indep } C !\Rightarrow A\; \text{indep } C$
>
> L'indépendance pair à pair ne suffit pas

*Illustration.* Dé 6 lancé 2 fois.

- $$A$$ : le lancer 1 est impair
- $$B$$ : le lancer 2 est impair
- $$C$$ : la somme des deux lancers est impaire

**Définition.** Probabilité conditionnelle : $$P(A | B) = {P(A \cap B) \over P(B)}$$

> $$A\; \text{indep } B \Leftrightarrow P(A | B) = P(A)$$
>
> Preuve :
 $$
 \begin{array}{lcl}
 A\; \text{indep } B & \Leftrightarrow & P(A \cap B) = P(A) . P(B) \\
 & \Leftrightarrow & {P(A \cap B) \over P(B)} = P(A) \\
 \end{array}
 $$
>
> Indépendants $$P(A \cap B) = P(A) P(B)$$ $$\neq$$ disjoints $$P(A \cap B) = 0$$.

*Exemple.* Calculer la probabilité que 1 arrive premier sachant que A arrive avant 2.

$$
P(A | B) = {P(A \cap B) \over P(B)} = {P(A) \over P(B)} = {2 / 6 \over 3 / 6} = {2 \over 3}
$$

**Formule des probabilités totales.** Soit $$B_1, ..., B_n$$ **partition** de $$\Omega$$ : $$B_i \cap B_j = \emptyset \forall i \neq j$$ et $$\bigcup\limits_{i=1}^{n} = \Omega$$)

$$
P(A) = \sum_{i = 1}^n P(A \cap B_i) = \sum_{i = 1}^n P(A | B_i)P(B_i)
$$

**Formules de Bayes.** $$P(A|B) = {P(B|A) P(A) \over P(B)}$$ (permet le passage de $$P(A|B)$$ vers $$P(B|A)$$ et inversement)

$$
P(A | B) = P(B | A)
$$
