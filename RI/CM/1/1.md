# RI 1 - Introduction

**Cours.** <https://hmul8r6b.imag.fr/doku.php/>

**Slide** : <https://hmul8r6b.imag.fr/lib/exe/fetch.php?media=accesinfoi-ii.pdf>

## Indexation et représentation

On s'intéresse au modèle **RI vectoriel** de 1965 (les modèles suivants utilisent les mêmes grands principes).

À partir des documents, l'étape principale consiste à créer le vocabulaire.

### Prétraitements linguistiques

#### Segmentation

**Segmentation.** Découper une chaîne de caractères en éléments syntaxiques ou mots (*tokenisation* en anglais).

**Occurences de mots.** Nombre de mots, plusieurs peuvent être identiques (dans un **vocabulaire** ils sont tous différents).

> *Remarque.* Difficultés avec chaque langue : par exemple en allemand il y a de très longs mots composés et en chinois il n'y a pas d'espaces.

> *Remarque.* En cours : NLTK Natural Language Toolkit. Fourni notamment des possibilités de segmentation.-

#### Normalisation

La **normalisation textuelle** rend les mots d’une même famille sous leur **forme canonique** (ponctuation, casse, accents, etc.).

**Algorithme de Porter.** Plusieurs règles avec une prémisse et une conclusion (par exemple avec `ies -> y` on a `daisies -> daisy`).

**Racinisation / Lemmatisation.** Troncature / Conjugaisons (par exemple)

**Anti-dictionnaire.** Mots inutiles à la recherche d'information (la, du, y, etc.).

**Corpus de documents.** Ensemble de documents dans la collection.

**Sac-de-mots** après racinisation + anti-dictionnaire de *l’information donne sens`a l’esprit* : *inform, don, sens, esprit*

### Deux lois de base en RI

La **loi de Heaps** et la **loi de Zipf**.

\newpage

### Exercices

Prenons la collection Wikipédia vue plus haut.

1. Proabilité *expérimentale* d'apparition du mot ***de*** dans la collection ?

> $$fc_{\text{exp}}(...)$$ est la fréquence expérimentale d'un mot.

Soit $M = \text{\# total d'occurences des mots} = 696 668 157$.

$$
P_{\text{exp}}(\text{"de"}) = {fc_{\text{exp}}(\text{"de"}) \over M} = {36 875 868 \over 696 668 157} = 0.053
$$

2. Estimation *théorique* du nombre d'apparition du mot ***de*** dans la collection ?

Soit $M_y = \text{\# total de mots} = 757 476$.

$$
\lambda = {M \over ln(M_y)} = {696 668 157 \over ln(757 476)} = 51 461 158.95
$$

$$
fc_{\text{th}}(\text{"de"}) = {\lambda \over rang(\text{"de"})} = {51 461 158.95 \over 1} = 51 461 158.95
$$

On a une marge d'erreur de l'ordre de $0,7$.

3. Quel serait le nombre moyen d’apparitions du mot le plus fréquent, ***de***, dans un document de taille 416 mots ?

$$
P_{exp}(\text{ de }) * 416 = 0,053 * 416 = 22
$$

\newpage

## Représentation vectorielle des documents

On associe à chaque document d'un vecteur $d$ dont la dimension correspond à la taille du vocabulaire ($d \in \mathbb{R}^{|V|}$). Chaque élément $w_i$ (*weight*) du vecteur $d$ correspond à l'importance ($w_i \in \mathbb{R}_+$) du mot $t_i$.

Ce poids $w_i$ se calcule :

- En multipliant un coefficient de **normalisation** $n_d$ *(facultatif)*
- Par la **caractérisation** du terme dans le **document** : est-ce que le mot est très présent à l'intérieur du document ? $p_{tf_{t_i,d}}$ (pour *term frequency*).
- Et par la **caractérisation** du terme dans le **corpus** : est-ce que le mot est très présent à l'intérieur de l'ensemble des documents ? $p_{df_{t_i}}$ (pour *document frequency*). Elle est **petite** quand le le mot est très présent dans les autres documents (et inversement).

$$
\forall d \in \mathcal{C}, \forall i \in \{1,...,V\},\;\;w_{id} = n_d \times p_{tf_{t_i,d}} \times p_{df_{t_i}}
$$

> - Un **terme peu discriminant** se trouve dans beaucoup de documents, ce qui ne va pas être très utile pour les filtrer : **$p_{df} \simeq 0$**
> - À l'inverse, un **terme très discriminant** se trouve dans très peu de documents, ce qui va être utile pour les filtrer : **$p_{df} >> 0$**. *Si on cherche un de ces documents à l'aide de ce terme on n'aura que peu de résultats dans lesquels fouiller.*

La normalisation vise à régler le problème de **taille de document** : donner autant d'importance aux documents courts qu'aux documents long (qui ont de par leur nature des poids plus grands).

En cours on choisit :

- $p_{tf_{t_i,d}} = tf_{t_i,d}$
- $p_{df_{t_i}} = idf_{t_i}( = ln {N \over df_{atom://teletype/portal/c20d57eb-5377-4fb6-ae84-f1bf2d05e02ct_i}})$, avec $N$ le nombre de documents dans le corpus.
- $n_d = 1$

**Index inversé.** Structure de données qui fait correspondre chaque terme du vocabulaire à la liste des documents qui le contiennent.

  - Très efficace pour la recherche
  - Construction avec une table de hachage ou un arbre
    - Si mémoire insuffisante dans la RAM, on applique l'algorithme *Blocked sort-based indexing* (ou BSBI)
