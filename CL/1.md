# CL 1 - Introduction

**Cours.** <http://www-clips.imag.fr/geod/User/laurent.besacier/CoursCL2011/>

**TPs.** <https://sites.google.com/view/william-n-havard/teaching/tps-polytech>

Langage utilisé en TP : **Python**

Intro modifiée sur git

$$
\text{Note} = \text{CC} * 0.25 + \text{Exam} * 0.75
$$

$$
\text{CC} = \texttt{mean(} \text{ TPs } \texttt{)}
$$

## Cours d'introduction

**Slide** : <http://www-clips.imag.fr/geod/User/laurent.besacier/CoursCL2011/1.pdf>

**NLP.** *Natural Language Processing*. Une sous-branche de l'IA.

> Outil de traduction plus intéressant que Google Traduction : DeepL (<https://www.deepl.com/>).

Principes généraux : analyse (déconstruction) et génération (construction).

Niveaux de descriptions : **lexical** - morphologique (mot), **syntaxique** (phrase), **sémantique** (sens), **pragmatique** (sens avec le contexte).

## 2.1 Représentation et codage de texte (Unicode)

**Slide** : <http://www-clips.imag.fr/geod/User/laurent.besacier/CoursCL2011/2.1.pdf>

Modèle en couches :

1. **Jeu de caractères abstraits** (*Abstract Character Repertoire*) : desciption de chacun des caractères (par exemple "A : lettre majuscule latine a") de chacun des systèmes d'écriture.
2. **Jeu de caractères codés** (*Coded Character Set*) : identifiant unique numérique donné à chaque caractère.
3. **Forme codée en mémoire** ou encodage (*Character Encoding Form*) : représentation en mémoire de chaque caractère.
4. *Les autres couches ne sont pas vues en cours.*

Unicode permet de faire des texte multi-écritures, il permet plusieurs encodages.

Un caractère peut avoir plusieurs codages : le "ç" a un code mais les codes de la cédille (**caractère combinatoire**) et du "c" mis bout à bout donnent aussi "ç".

*Exemples d'encodages.*

- **US-ASCII** : premier encodage populaire, caractères sans accents
- **UTF-8** : taille variable, léger la plupart du temps, le plus utilisé sur Internet
- **UTF-16** : 16 bits, assez utilisé
- **UTF-32** : 32 bits, complet mais lourd

Unicode et Python : <https://docs.python.org/2/howto/unicode.html>

**Normalisation.** Pour comparer deux chaînes de caractères il ne faut pas comparer les octets en mémoire, il faut les normaliser avant pour qu'elles utilisent un même encodage.

## 2.2 Théorie de l'information et probabilités

**Slide** : <http://www-clips.imag.fr/geod/User/laurent.besacier/CoursCL2011/2.2.pdf>

**Loi de Zipf.** $f \times r = k$ avec $f$ le nombre d'occurences et $r$ le rang *(comme en cours de MRI)*

**Estimation par maximum de vraisemblance.** $p(w) = {\text{count}(w) \over \sum_{w'} \text{count}(w')}$

*Formalisation.* $P(W = w) = p(w)$ : si je tire un mot au hasard, quelle est la probabilité que ce mot soit $w$ ?

**Probabilités conjointes.** $p(w_1, w_2) = {\text{count}(w_1, w_2) \over \sum_{w'_1, w'_2} \text{count}(w'_1, w'_2)}$ *(pour les bigrammes ici)*

**Probabilités conditionnelles.** $p(w_2 | w_1) = {p(w_1, w_2) \over p(w_1)}$ et si $w_1$ et $w_2$ indépendantes on a $p(w_2 | w_1) = p(w_2)$ *(peu probable)*

**Règle 1, "chain rule".** $p(w_1, w_2, w_3) = p(w_1) p(w_2 | w_1) p(w_3 | w_1, w_2)$ etc. On va plutôt utiliser le $\log$ des probabilités (on aura des sommes plutôt que des produits du coup) pour éviter de se retrouver avec des valeurs très vite très petites et difficilement stockables, de plus le calcul sera ainsi plus rapide.

**Règle 2, "Bayes".** $p(x|y) = {p(y|x) p(x) \over p(y)}$

### Rappels

**Espérance.** $E(X) = \sum_x p(x) x = \mu$

**Variance.** $\text{Var}(X) = E((X-E(X))^2) = E(X^2) - E^2(X)$

**Écart-type.** $\text{Var}(X) = \sigma^2$

**Entropie.** $H(X) = \sum_x -p(x) \log_2 p(x)$

L'entropie est utilisée pour calculer la prédictabilité des modèles de langues : plus l'entropie est faible, plus la capacité prédictive est élevée. Elle permet également de trouver la taille d'un codage optimal. Par exemple avec Huffman, on obtient un codage binaire de longueur variable dont la longueur moyenne pour encoder un caractère est proche de l'entropie.

> Les **modèles unigrammes** ne prennent pas en compte l'ordre des mots, ce sont comme des sacs de mots qui ont chacun une probabilité d'apparition mais on ne se préoccupe pas du mot qui précède ou suit chacun des mots d'une phrase.

### Évènements inconnus

**Première approche simple.** Pour les évènements inconnus on leur donne une probabilité $\epsilon$ proche de zéro mais non nulle afin de ne pas mettre à zéro les phrases qui les contiennent.

**Deuxième approche plus poussée.** On peut avoir deux types de zéros : pour les mots inconnus et pour les n-grammes inconnus.

Si on donne des valeurs non nulles ce n'est plus une estimation par maximum de vraisemblance, la somme ne vaut pas $1$ : pour contourner ce problème, on "tasse" les probabilités connues à $0,99$ (par exemple) et on se réserve $0,01$ (toujours dans l'exemple) pour les évènements inconnus, ainsi la somme vaut toujours $1$.
