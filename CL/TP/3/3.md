% Communication Langagière -- TP3
% Claire VELUT et Mica MURPHY
% Mercredi 5 février 2020

# TITRE

## Exercice 1

On coupe les fichiers en 2 parties avec un nombre de lignes égal à l'aide de la commande suivante, comme on a remarqué que tous les fichiers contiennent 20000 lignes. *Voir début du script `txt`.*

```bash
for f in ??.txt; do;
  head -n 10000 $f > train.${f%.txt};
  tail -n 10000 $f > test.${f%.txt};
done
```

### Q1

On doit utiliser des textes différents pour l'apprentissage et le test, car si on donne un même texte à l'apprentissage et au test, le modèle peut avoir seulement "mémorisé" la bonne réponse.

## Exercice 2

On fait pour chacun des fichiers les deux commandes du sujet. *Voir fin du script `txt`.*

```bash
for f in ??.txt; do;
  ngram-count -order 3 -text train.${f%.txt} -write ${f%.txt}3g.bo -write-vocab ${f%.txt}.voc;
  ngram-count -read ${f%.txt}3g.bo -lm ${f%.txt}3g.BO -gt2min 1 -gt3min 2;
done
```

### Q1

- Les fichiers `.voc` contiennent tout le vocabulaire du texte auquel il sont associés.
- Les fichiers `.bo` contiennent les unigrammes, bigrammes et trigrammes présents dans le texte et leur nombre d'apparitions.
- Les fichiers `.BO` contiennent une valeur homogène au logarithme de la probabilité d'apparitions des  unigrammes, bigrammes et trigrammes et un poids de correction multiplicateurs pour conserver une plage de probabilités à donnée aux mots qui n'apparaissent pas dans le corpus pour qu'ils n'aient pas une probabilité nul.


### Q2
Le symbole `<s>` correspond au caractère de début de phrase et `</s>` au caractère de fin de phrase.

\newpage

### Q3

D'après la page `ngram-count -help`, les options `-gt2min` et `-gt3min` servent respectivement à `lower 2gram discounting cutoff` et à `lower 3gram discounting cutoff`. `gt` correspond à Good-Turing (un estimateur de fréquence qui lisse les probabilités), `2` et `3` correspondent au `n` des n-grammes et `min` correspond au paramètre de Good-Turing qui est modifié. En gros, on ne prend que les 2-grammes et les 3-grammes qui apparaissent au moins un certain nombre de fois (le paramètre de l'option).

### Q4

Pour trouver le trigramme le plus fréquent dans toutes les langues, on a cherché déjà tous les trigrammes (avec `grep`), puis on a trié numériquement par rapport à la dernière colonne en ordre décroissant (avec `sort`) et on a récupéré le premier résultat (avec `head`).

```bash
for f in ??.txt; do;
  echo ${f%.txt};
  grep '^\w\+\s\+\w\+\s\+\w\+\s\+' ${f%.txt}3g.bo | sort -r -n -u -k 4 | head -n 1;
done
```

Voici le résultat, on a une ligne avec le nom de la langue (sur deux caractères) et une ligne avec le trigramme le plus fréquent pour chaque langue. Par exemple en français c'est "monsieur le président" le trigramme le plus fréquent avec 772 apparitions.

![Trigrammes les plus fréquents dans toutes les langues](img/trigram.png)

\newpage

## Exercice 3

On a d'abord affiché une ligne avec les langues, puis sur chaque ligne la langue suivie des valeurs. Pour récupérer uniquement la valeur qui nous intéressant dans le résultat de la commande `ngram`, on a récupéré d'abord les lignes contenant `ppl` (avec `grep`), puis on a supprimé tout ce qui se trouve avant et après la valeur (avec `sed`). *Voir script `matrix`.*

```bash
# Affichage de la première ligne
echo -n "\t";
for f in ??.txt; do;
    echo -n "${f%.txt}\t";
done;
echo;

for f in ??.txt; do;
    # Affichage de la première colonne
    echo -n "\r${f%.txt}\t";
    for g in test.*; do;
      # Affichage des valeurs
      printf "%.2f\t" $(ngram -lm ${f%.txt}3g.BO -ppl $g | grep 'ppl' | sed 's/.*ppl= \(.*\) ppl1.*$/\1/');
    done;
    echo;
done
```

### Q1

La perplexité permet de voir la prédictibilité du modèle. Plus c'est faible, plus le modèle sera capable de prédire correctement les mots qui suivent. En moyenne la perplexité d'une langue sur un modèle de la même langue est plus faible que celle d'une langue sur un modèle d'une autre langue.

![Matrice des valeurs](img/ordre3.png)

### Q2

En observant les valeurs de perplexité d'un textes par rapport aux différents modèles de langues, on peut estimer la langue du texte en choisissant la combinaison avec la perplexité la plus faible.

En pratique, en regardant la colonne correspondant au fichier testé et en regardant la valeur la plus faible dans cette colonne on trouve la ligne de la langue.

\newpage

## Exercice 4

On a utilisé le script `splitletters.awk` pour séparer chacun des octets par un espace (avec `cat` et `awk`). On a ainsi des fichiers `S??.txt`, `Strain.??` et `Stest.??`. On a choisi `S` pour *split*. *Voir début du script `txtS`.*

```bash
for f in ??.txt; do;
  cat $f | awk -f ../splitletters.awk > S$f;
  head -n 10000 S$f > Strain.${f%.txt};
  tail -n 10000 S$f > Stest.${f%.txt};
done
```

### Q1

Le modèle de caractère permet de détecter des erreurs de typographie, mais aussi des fautes d'orthographe.

- Avantage : le pattern est plus fin, propre aux langues et il y a un nombre fini de caractères donc il y a beaucoup plus de chances qu'ils soient tous dans le corpus de départ même si le corpus n'est pas très grand.
- Inconvénient : on ne teste pas le sens des n-grammes (suites de mots logiques ou non).

On refait comme avant pour générer les fichiers `.voc`, `.bo` et `.BO` et pour l'affichage de la matrice. On a ainsi des fichiers `S??.voc`, `S??3g.bi` et `S??3g.BO`. Voir fin du script `txtS` pour le code ci-dessous et voir le script `matrixS` pour l'affichage de la matrice.

```bash
# Fichiers .voc, .bo et .BO
for f in S??.txt; do;
ngram-count -order 3 -text Strain.${${f%.txt}:1} -write ${f%.txt}3g.bo -write-vocab ${f%.txt}.voc;
ngram-count -read ${f%.txt}3g.bo -lm ${f%.txt}3g.BO -gt2min 1 -gt3min 2;
done
```

Ce qui donne le résultat ci-dessous.

![Matrice des valeurs (caractère par caractère)](img/matrice_split.png)

\newpage

### Q2

On fait la même chose en remplaçant les espaces par `<SPACE>` et avec comme préfixe `R` pour *replace* plutôt que `S`. *Voir les scripts `txtR` et `matrixR`.*

Ce qui donne le résultat ci-dessous. On remarque que toutes les valeurs sont **plus faibles** que pour le résultat précédent (notamment pour la "bonne" valeur), cela se voit particulièrement sur la première colonne : avant on avait des valeurs à environ 8 pour le Bulgare et environ 45 pour le reste (sauf 2 exceptions) alors que maintenant on a une valeur à environ 6 pour le Bulgare et à environ 8 pour le reste (sauf les mêmes 2 exceptions). Globalement aussi, **l'écart s'est réduit** entre la "bonne" valeur et les autres. On devinera tout de même avec une plus grande certitude ave le second modèle (avec les `<SPACE>`).

![Matrice des valeurs (caractère par caractère en comptant l'espace)](img/matrice_split_space.png)

## Extension

### 1

Les paires de langues qui sont les plus faciles à distinguer les unes des autres sont les paires de langues avec une grande perpléxité. En effet cela veut dire que les n-grammes du textes das la langue A sont peu prèsent dans le modele de langue B donc qu'il est faciles de distingué un texte en langue B d'un texte en langue A. Inversement les paires de langues les plus difficiles à distinguer les une des autres sont les paires de langues avec une perplexité la plus faible. Dans ce cas il sera difficile de determiner en quelle langue est un texte car la probabilité d'un n-gramme pour la langue A est proche de celle de la langue B.

### 2

Nous avons réalisé les matrices de perplexité pour des modèles d'ordre 3, 4, 5. On peut remarquer que les perplexités sont plus faibles pour les n-grammes d'ordre 4 et encore plus faibles pour les n-grammes d'ordre 5. En effet, plus l'ordre est grand plus le modèle est précis. Dans notre cas, on n'a pas pu le vérifier en pratique (les trois résultats sont identiques, l'ordre n'a pas l'air d'être pris en compte).
