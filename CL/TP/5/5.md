% Communication Langagière -- TP5
% Claire VELUT et Mica MURPHY
% Mercredi 19 février 2020

# Traduction automatique

## 1. Préambules

Nous avons installé les outils sur une seule des deux machines par manque de place.

## 2. Préparation des données

### Q1

Les fichiers `IWSLT10_BTEC.train.en.txt` et `IWSLT10_BTEC.train.fr.txt` contiennent des phrases en anglais et en français ainsi qu'un identifiant.

### Q2

On a tokenizé les fichiers `.clean` dans les fichiers `.tok` avec : `tokenizer.perl -l fr -lc < BTEC-en-fr/XXXX/IWSLT10_BTEC.XXXX.LANG.clean.txt > BTEC-en-fr/XXXX/IWSLT10_BTEC.XXXX.LANG.tok.txt`  (`XXXX` valant `pred`, `test` et `train` ; `LANG` valant `fr` et `en`). Ainsi, toutes les ponctuations ont un espace avant et après, et des caractères spéciaux ont étaient insérés pour les apostrophes. L'intéret est de séparer les mots.

### Q3

- Les fichiers dans `train` permettent d'entraîner notre modèle.
- Les fichiers dans `test` vont permettre de tester notre modèle après qu'il ait été entraîné.
- Les fichiers dans `dev` permettront de faire l'équivalent d'un test pendant l'entraînement, ce qui permettra d'éviter "l'over fitting", le sur entrainement.

## 3. Création de TA

On a fait le préprocessing avec :

```bash
onmt_preprocess -train_src ./BTEC-en-fr/train/IWSLT10_BTEC.train.fr.tok.txt -train_tgt ./BTEC-en-fr/train/IWSLT10_BTEC.train.en.tok.txt -valid_src ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.fr.tok.txt -valid_tgt ./BTEC-en-fr/dev/IWSLT10.devset1_CSTAR03.en.tok.txt -save_data ./BTEC-en-fr/BTEC-en-fr --src_words_min_frequency 1 --tgt_words_min_frequency 1
```
### Q4

D'après la commande précédente, la langue source est le français et la langue cible est l'anglais. Le modèle va dont apprendre à traduire du français vers l'anglais.

### Q5

Les lignes `src vocab size: 9913` et `tgt vocab size: 8146`  nous donne la taille du vocabulaire source et cible.

### Q6

Nous avons fait la traduction avec : `onmt_translate -model vanilla/rnn-basic_step_XXXX.pt -src test/IWSLT09_BTEC.testset.fr.tok.txt -output pred/IWSLT09_BTEC.pred.XXXX.txt -verbose` (`XXXX` valant $625$, $1250$, $1875$ et $2000$).

La valeur `ppl` correspond à la perplexité.
Il vaut mieux que cette valeur soit la plus faible possible, plus la perplexité est grande, moins le modèle de langue contient d'informations.
La valeur `acc` correspond à `l'accurency`, soit la précision de la traduction.
Plus cette valeur est grande mieux c'est.

### Q7

Nous avons regardé la documentation de la commande d'entrainement : <https://opennmt.net/OpenNMT-py/options/train.html>

- `train_steps: 2000` : nombre d'étapes d'apprentissage
- `valid_steps: 625` : effectue la validation toutes les X étapes
- `enc_layers: 1` : nombre de couches dans l'encodeur
- `dec_layers: 1` : nombre de couches dans le décodeur
- `enc_rnn_size: 256` : taille des états cachés des *recurrent neural networks* de l'encodeur
- `dec_rnn_size: 256` : taille des états cachés des *recurrent neural networks* du décodeur
- `batch_size: 32` : taille maximale de lots pour l'entraînement

## 4. Évaluation d'un modèle de TA

Nous avons regardé la documentation pour calculer le score BLEU :
<https://opennmt.net/OpenNMT-py/extended.html>

Nous avons entraîné notre modèle avec : `onmt_train --config config-rnn-basic.yml`.

### Q8

Un score BLEU évalue la qualité d'un texte traduit automatiquement et est toujours compris entre 0 et 1 (ou 0 et 100 quand c'est considéré comme un pourcentage). Il vaut mieux que la valeur soit plus proche de 1 car cela indique que la traduction proposé est plus proche d'un texte de référence.

### Q9

Nous avons calculé le score BLEU avec : `perl multi-bleu.perl BTEC-en-fr/test/IWSLT09_BTEC.testset.en.tok.txt < BTEC-en-fr/pred/IWSLT09_BTEC.pred.XXXX.txt` (`XXXX` valant $625$, $1250$, $1875$ et $2000$).

- Modèle après 625 étapes : score BLEU =  $7.41$
- Modèle après 1250 étapes : score BLEU = $10.79$
- Modèle après 1875 étapes : score BLEU = $18.28$
- Modèle après 2000 étapes : score BLEU = $18.57$

Le modèle final (après 2000 étapes) a le meilleur score BLEU.

## 5. Optimisations des paramètres

### Q10 (Tom)

Nous avons augmenté le nombre de `training steps` jusqu'à 10000.
Avec plus d'étapes d'apprentissage le score BLEU augmente jusqu'à environ 33, où il a l'air d'atteindre un palier où il stagne.

Nombre de steps | Score BLEU
----------------|-----------
$625$           | $7.41$
$1250$          | $10.79$
$1875$          | $18.28$
$2500$          | $20.46$
$3125$          | $25.44$
$3750$          | $27.50$
$4375$          | $28.27$
$5000$          | $30.93$
$5625$          | $29.66$
$6250$          | $32.13$
$6875$          | $30.54$
$7500$          | $32.81$
$8125$          | $33.44$
$8750$          | $32.67$
$9375$          | $33.33$
$10000$         | $33.57$

Pour la suite du TP nous avons repris un nombre de `training steps` à 2000 sinon les temps de calculs étaient trop longs.

### Q11 (Alexis)

Nous avons augmenté le nombre de couches de l'encodeur et du décodeur à $2$.

Plus le réseau est profond, plus il lui faut de steps car sinon il n'a pas assez de données. Pour 2000 steps on obtient un score BLEU de $13.58$, ce qui est en effet moins bon que notre score précédent de $18.57$ avec $1$ couche pour l'encodeur et le décodeur. Le nombre de steps est donc insuffisant pour ce nombre de couches.

### Q12 (Claire)

Nous avons fait varier le nombre d'unités pour l'encodeur et le décodeur et nous les avons mis à $512$.

On remarque que lorsqu'on a peu d'étapes, si on augmente le nombre d'unités alors le score BLEU est meilleur ($8.56 > 7.41 \text{ et } 8.20$), en revanche si on a beaucoup d'étapes alors le score est moins bon ($13.33 < 18.57 \text{ et } 18.50$). Nous voudrons avoir un nombre de steps un minimum grand, alors il faudra prendre un grand nombre d'unités.

Nombre de steps | Score BLEU ($512$ unités)| Score BLEU ($256$ unités)| Score BLEU ($128$ unités)
----------------|--------------------------|--------------------------|--------------------------
$625$           | $8.20$                   | $7.41$                   | $8.56$
$1250$          | $13.84$                  | $10.79$                  | $9.55$
$1875$          | $17.65$                  | $18.28$                  | $16.64$
$2000$          | $18.50$                  | $18.57$                  | $13.33$

### Q13 (Tom, TODO)

Nous avons ajouté un méchanisme d’attention en changeant le paramètre de `global_attention` en le passant de `none` à `mlp`.

Nombre de steps | Score BLEU avec `mlp`| Score BLEU avec `none`
----------------|----------------------|-----------------------
$625$           | $2.67$               | $7.41$
$1250$          | $5.91$               | $10.79$
$1875$          | $6.08$               | $18.28$
$2000$          | $16.14$              | $18.57$


On remarque que le score avec `none` est toujours meilleur que

## Q14

Le beam search est une algorithme de recherche approximative basée sur un parcours en largeur afin choisir les meilleurs traductions parmi tous les candidats possibles de manière efficace.

Par défaut la taille du beam search est de $5$ (<https://opennmt.net/OpenNMT-py/options/translate.html>) et elle correspond au nombre de mots à garder en mémoire à chaque étape pour tester les possibilités, soit le nombre de fils pour chaque niveau du graphe.

## Q15 (TODO)

Le score BLEU augmente un peu mais pas beaucoup.
ex: pour le modèle 1875
avec beam=1 -> 14.14
avec beam=10 -> 16.69

> Changez la taille du beam en la faisant varier de 1 à 10. Comment évolue le score BLEU ?
> Il est possible de regarder les traductions alternatives qu’aurait pu choisir le modèle. Pour cela il suffit d’utiliser la commande `--n_best X`, où X est le nombre de solutions alternatives que vous souhaitez consulter.

## Q16 (TODO)

> Générez un fichier de traduction en utilisant votre meilleur modèle en conservant les 5 phrases les plus probables. Que pensez-vous des 5 traductions proposées ? Sont-elles nécessairement moins bonnes que la traduction attendue ?

## Notes sur les résultats attendus

> BLEU minimum 40, voire 50 avec des bons paramètres

> Pour avoir de la puissance de calcul : Google collab ou AWS (Amazon Web Services)

.
